# Override precision and add explicit optimizer / scheduler settings.
# Combine with config/base.yaml (loaded by default) and a model config.
trainer:
  precision: "16-mixed"

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 3e-4
    weight_decay: 1e-2

lr_scheduler:
  class_path: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  init_args:
    T_0: 391
    T_mult: 2
    eta_min: 1e-5
